{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Packages\n",
    "import os\n",
    "import re\n",
    "import scipy\n",
    "import spacy\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mlflow\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertModel, BertConfig, BertForSequenceClassification\n",
    "Tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 22\n",
    "EPOCHS = 6\n",
    "MAX_LEN = 500\n",
    "TRAIN_SIZE = 0.75\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 16\n",
    "LEARNING_RATE = 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLP_Processor():\n",
    "    '''\n",
    "    This is a class used to process dataset for NLP models\n",
    "    '''\n",
    "    def __init__(self, folders, categories, path):\n",
    "        # self.__folders = folders\n",
    "        # self.__categories = categories\n",
    "        # self.__path = path\n",
    "        self.source = self.get_dataframe(folders, categories, path)\n",
    "        self.bert_data = self.string_cleaner(self.source.copy(), \"Documents\", \"BERT\")\n",
    "        self.wv_data = self.string_cleaner(self.source.copy(), \"Documents\", \"Word2Vec\")\n",
    "        \n",
    "    def read_data(self, path, label, texts, labels):\n",
    "        '''\n",
    "        Read txt files under a specific path\n",
    "        path:   input, string\n",
    "                the full path of folder where txt files are stored\n",
    "        label:  input, string\n",
    "                the label for txt files under path\n",
    "        texts:  input, list\n",
    "                a list to store all imported strings\n",
    "        labels: input, list\n",
    "                a list to store the labels for each string\n",
    "        '''\n",
    "        files = os.listdir(path)\n",
    "        for file in files:\n",
    "            labels.append(label)\n",
    "            full_path = os.path.join(path,file)\n",
    "            with open(full_path, encoding='gb18030', errors='ignore') as file_obj:\n",
    "                content = file_obj.read()\n",
    "                texts.append(content) \n",
    "                \n",
    "    def get_dataframe(self, folders, categories, path):\n",
    "        '''\n",
    "        This is a function used to get data from different folders with diffrent categories\n",
    "        All the folders are stored under one path\n",
    "        The output of this function is a dataframe that contains all strings\n",
    "        folders:    input, list\n",
    "                    the list of folders\n",
    "        categoreis: input, list\n",
    "                    the list of categories corresponding to each folder\n",
    "        path:       the full path where the folders are stored\n",
    "        '''\n",
    "        texts = list()\n",
    "        labels = list()\n",
    "        for i in range(0, len(folders)):\n",
    "            folder_path = path + '/' + folders[i]\n",
    "            label = categories[i]\n",
    "            self.read_data(folder_path, label, texts, labels)\n",
    "        doc_dict = {\"Documents\":texts, \"Categories\":labels}\n",
    "        df = pd.DataFrame(doc_dict, columns=[\"Documents\", \"Categories\"])\n",
    "        return df   \n",
    "    \n",
    "    def string_cleaner(self, df, col, mod):\n",
    "        '''\n",
    "        Clean the strings\n",
    "        df:     input, pdd.DataFrame\n",
    "                where the strings are stored\n",
    "        col:    input, string\n",
    "                indicate which column is required cleaning\n",
    "        mod:    input, string\n",
    "                Prepare dataset for different vectorizing method: Bert, Word2Vec.etc\n",
    "        '''\n",
    "        df[col]=df[col].apply(lambda x:x.replace(\"\\n\",\" \"))\n",
    "        df[col]=df[col].apply(lambda x:re.sub(\"[^A-Za-z0-9(),.!?\\'\\`]\", \" \", x))\n",
    "        if mod != 'BERT':  \n",
    "            df[col]=df[col].apply(lambda x:re.sub(\"\\'s\", \" \\'s \", x))\n",
    "            df[col]=df[col].apply(lambda x:re.sub(\"\\'ve\", \" \\'ve \", x))\n",
    "            df[col]=df[col].apply(lambda x:re.sub(\"n\\'t\", \" n\\'t \", x))\n",
    "            df[col]=df[col].apply(lambda x:re.sub(r\"\\'re\", \" \\'re \", x))\n",
    "            df[col]=df[col].apply(lambda x:re.sub(r\"\\'d\", \" \\'d \", x))\n",
    "            df[col]=df[col].apply(lambda x:re.sub(r\"\\'ll\", \" \\'ll \", x))\n",
    "            df[col]=df[col].apply(lambda x:re.sub(r\",\", \" , \", x))\n",
    "            df[col]=df[col].apply(lambda x:re.sub(r\".\", \" . \", x))\n",
    "            df[col]=df[col].apply(lambda x:re.sub(r\"!\", \" ! \", x))\n",
    "            df[col]=df[col].apply(lambda x:re.sub(r\"\\?\", \" ? \", x)) \n",
    "            df[col]=df[col].apply(lambda x:re.sub(r\"\\(\", \" ( \", x))\n",
    "            df[col]=df[col].apply(lambda x:re.sub(r\"\\)\", \" ) \", x))\n",
    "        df[col]=df[col].apply(lambda x:re.sub(r\"\\s{2,}\", \" \", x))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data and get the dataset prepared\n",
    "folders = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
    "categories = ['Business', 'Entertainment', 'Politics', 'Sport', 'Technology']\n",
    "path = '/home/hzhan10/NLP Research'\n",
    "\n",
    "processor = NLP_Processor(folders, categories, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(train_size, df, seed):\n",
    "    '''\n",
    "    Split dataset into training and testing samples\n",
    "    train_size:     input, float\n",
    "                    the size of training set (a portion)\n",
    "    df:             input, pd.DataFrame\n",
    "                    all data samples\n",
    "    seed:           input, int\n",
    "                    random seed\n",
    "    '''\n",
    "    train_df = df.sample(frac=train_size, random_state=seed)\n",
    "    test_df = df.drop(train_df.index).reset_index(drop=True)\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (2225, 2)\n",
      "TRAIN Dataset: (1669, 2)\n",
      "TEST Dataset: (556, 2)\n"
     ]
    }
   ],
   "source": [
    "bert_data = processor.bert_data\n",
    "train_df, test_df = data_split(TRAIN_SIZE, bert_data, SEED)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(bert_data.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_df.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_creator(category, df, col):\n",
    "    '''\n",
    "    Create balance binary dataset, 1 for selected category, 0 for other cats\n",
    "    category:   input, string\n",
    "                the label of a specific topic\n",
    "    df:         input, pd.DataFrame\n",
    "    col:        input, string\n",
    "                the name of column where labels are stored\n",
    "    '''\n",
    "    df_1 = df.copy()\n",
    "    cat_1 = df_1[df_1[col] == category]\n",
    "    df_1 = df_1.drop(cat_1.index).reset_index(drop=True)    \n",
    "    \n",
    "    # preparation for oversampling if sample <500\n",
    "    cat_2 = df_1.sample(n=len(cat_1),random_state=22)        \n",
    "    df_set = pd.concat([cat_1, cat_2])\n",
    "    # df_set.reset_index(inplace=True, drop=True)\n",
    "    df_set[col][df_set[col] != category]=0\n",
    "    df_set[col][df_set[col] == category]=1\n",
    "    df_set = df_set.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    return df_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encoder(df, col, categories):\n",
    "    from sklearn import preprocessing\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(categories)\n",
    "    df[col]=le.transform(df[col])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business Train Dataset: (778, 2)\n",
      "Entertainment Train Dataset: (582, 2)\n",
      "Policy Train Dataset: (590, 2)\n",
      "Sports Train Dataset: (770, 2)\n",
      "Technology Train Dataset: (618, 2)\n"
     ]
    }
   ],
   "source": [
    "busi_train = binary_creator('Business', train_df, 'Categories')\n",
    "print(\"Business Train Dataset: {}\".format(busi_train.shape))\n",
    "\n",
    "ent_train = binary_creator('Entertainment', train_df, 'Categories')\n",
    "print(\"Entertainment Train Dataset: {}\".format(ent_train.shape))\n",
    "\n",
    "pol_train = binary_creator('Politics', train_df, 'Categories')\n",
    "print(\"Policy Train Dataset: {}\".format(pol_train.shape))\n",
    "\n",
    "sp_train = binary_creator('Sport', train_df, 'Categories')\n",
    "print(\"Sports Train Dataset: {}\".format(sp_train.shape))\n",
    "\n",
    "tech_train = binary_creator('Technology', train_df, 'Categories')\n",
    "print(\"Technology Train Dataset: {}\".format(tech_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business Test Dataset: (242, 2)\n",
      "Entertainment Test Dataset: (190, 2)\n",
      "Policy Test Dataset: (244, 2)\n",
      "Sports Test Dataset: (252, 2)\n",
      "Technology Test Dataset: (184, 2)\n"
     ]
    }
   ],
   "source": [
    "busi_test = binary_creator('Business', test_df, 'Categories')\n",
    "print(\"Business Test Dataset: {}\".format(busi_test.shape))\n",
    "\n",
    "ent_test = binary_creator('Entertainment', test_df, 'Categories')\n",
    "print(\"Entertainment Test Dataset: {}\".format(ent_test.shape))\n",
    "\n",
    "pol_test = binary_creator('Politics', test_df, 'Categories')\n",
    "print(\"Policy Test Dataset: {}\".format(pol_test.shape))\n",
    "\n",
    "sp_test = binary_creator('Sport', test_df, 'Categories')\n",
    "print(\"Sports Test Dataset: {}\".format(sp_test.shape))\n",
    "\n",
    "tech_test = binary_creator('Technology', test_df, 'Categories')\n",
    "print(\"Technology Test Dataset: {}\".format(tech_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CustomDataset class to prepare data for Pytorch\n",
    "class CustomDataset(Dataset):\n",
    "    '''\n",
    "    Inherit Dataset class from torch and re-write its methods\n",
    "    '''\n",
    "    # def __init__(self, dataframe, text, target, tokenizer, max_len):\n",
    "    def __init__(self, dataframe, text, target, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.comment_text = dataframe[text]\n",
    "        self.targets = dataframe[target]\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comment_text)\n",
    "\n",
    "    def __getitem__(self, index): \n",
    "        comment_text = str(self.comment_text[index]) #certain sentence\n",
    "        comment_text = \" \".join(comment_text.split()) #split and then join\n",
    "        # What are the outputs of tokenizer.encode_plus()\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            comment_text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=False,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source is the return of getitem\n",
    "# https://pytorch.org/docs/master/generated/torch.nn.utils.rnn.pad_sequence.html\n",
    "def pad_seq(seq, max_batch_len, pad_value):\n",
    "    '''\n",
    "    Pad inputs for BERT\n",
    "    '''\n",
    "    # IRL, use pad_sequence\n",
    "    return seq + (max_batch_len - len(seq)) * [pad_value]\n",
    "\n",
    "# Create batches \n",
    "def collate_batch(batch, tokenizer):\n",
    "    '''\n",
    "    Build batches for Pytorch\n",
    "    '''\n",
    "    batch_inputs = list()\n",
    "    batch_attention_masks = list()\n",
    "    batch_token_type_ids=list()\n",
    "    labels = list()\n",
    "    # find the max length of the mini batch\n",
    "    max_size = max([len(ex['ids']) for ex in batch])\n",
    "    for item in batch:\n",
    "        # apply padding at the mini batch level\n",
    "        batch_inputs += [pad_seq(item['ids'].tolist(), max_size, tokenizer.pad_token_id)]\n",
    "        batch_attention_masks += [pad_seq(item['mask'].tolist(), max_size, 0)]\n",
    "        batch_token_type_ids += [pad_seq(item['token_type_ids'].tolist(), max_size, 0)]\n",
    "        labels.append(item['targets'].tolist())\n",
    "    # expected Transformers input format (dict of Tensors)\n",
    "    return {\"ids\": torch.tensor(batch_inputs, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(batch_attention_masks, dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(batch_token_type_ids, dtype=torch.long),\n",
    "            \"targets\": torch.tensor(labels, dtype=torch.long)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "Test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    '''\n",
    "    Creating the customized model, by adding a drop out and a dense layer on top of distil bert \n",
    "    to get the final output for the model. \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        # ,return_dict=False\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased',return_dict=False)\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 1)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Binary_Classifier():\n",
    "    def __init__(self, train_set, test_set, tokenizer, device):\n",
    "        self.train_set = train_set\n",
    "        self.test_set = test_set\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.model = BERTClass()\n",
    "        self.model.to(device) \n",
    "    \n",
    "    def set_optimizer(self, learning_rate):\n",
    "        self.optimizer = torch.optim.Adam(params=self.model.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def loss_fn(self, outputs, targets):\n",
    "        return torch.nn.BCEWithLogitsLoss(pos_weight=torch.FloatTensor([1.7]))(outputs.flatten(), targets.type_as(outputs))\n",
    "        \n",
    "    def set_dataset(self, text, target, max_len):\n",
    "        self.training_set = CustomDataset(self.train_set, text, target, self.tokenizer, max_len)\n",
    "        self.testing_set = CustomDataset(self.test_set, text, target, self.tokenizer, max_len)\n",
    "        \n",
    "    def set_dataloader(self, func, train_params, test_params):\n",
    "        self.training_loader = DataLoader(self.training_set, collate_fn=lambda batch: func(batch, self.tokenizer), **train_params)\n",
    "        self.testing_loader = DataLoader(self.testing_set, collate_fn=lambda batch: func(batch, self.tokenizer), **test_params)\n",
    "        \n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        for _,data in enumerate(self.training_loader, 0):\n",
    "            ids = data['ids'].to(self.device, dtype=torch.long)\n",
    "            mask = data['mask'].to(self.device, dtype=torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(self.device, dtype=torch.long)\n",
    "            targets = data['targets'].to(self.device, dtype=torch.long)\n",
    "\n",
    "            outputs = self.model(ids, mask, token_type_ids)\n",
    "            # remove cache of gradient\n",
    "            self.optimizer.zero_grad() \n",
    "            loss = self.loss_fn(outputs, targets) \n",
    "            # print(f'epoch: {epoch}, Loss:  {loss.item()}, {_}')\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        print(\"Model Training Finished!\")   \n",
    "        \n",
    "    def validation(self, data_loader):\n",
    "        self.model.eval()\n",
    "        fin_targets=[]\n",
    "        fin_outputs=[]\n",
    "        with torch.no_grad():\n",
    "            for cnt, data in enumerate(data_loader, 0):\n",
    "                ids = data['ids'].to(self.device, dtype=torch.long)\n",
    "                mask = data['mask'].to(self.device, dtype=torch.long)\n",
    "                token_type_ids = data['token_type_ids'].to(self.device, dtype=torch.long)\n",
    "                targets = data['targets'].to(self.device, dtype=torch.long)\n",
    "                outputs = self.model(ids, mask, token_type_ids)\n",
    "                fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "                fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "        fin_outputs = [pred[0] for pred in fin_outputs]\n",
    "        # loss = loss_fn(fin_outputs, fin_targets) \n",
    "        return fin_outputs, fin_targets \n",
    "    \n",
    "    def epoch_train(self, epochs, filename):\n",
    "        from sklearn.metrics import log_loss\n",
    "        train_losses = list()\n",
    "        test_losses = list()\n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            train_outputs, train_targets = self.validation(self.training_loader)\n",
    "            train_loss = log_loss(train_targets, train_outputs) \n",
    "            \n",
    "            test_outputs, test_targets = self.validation(self.testing_loader)\n",
    "            test_loss = log_loss(test_targets, test_outputs) \n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            \n",
    "            if epoch == 4:\n",
    "                torch.save(self.model, '/home/hzhan10/NLP Research/' + filename + '.pt')\n",
    "            \n",
    "            print(str(epoch) + ' epoch finished!')\n",
    "        return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training for each document category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training Finished!\n",
      "0 epoch finished!\n",
      "Model Training Finished!\n",
      "1 epoch finished!\n",
      "Model Training Finished!\n",
      "2 epoch finished!\n",
      "Model Training Finished!\n",
      "3 epoch finished!\n",
      "Model Training Finished!\n",
      "4 epoch finished!\n",
      "Model Training Finished!\n",
      "5 epoch finished!\n"
     ]
    }
   ],
   "source": [
    "busi_classifier = BERT_Binary_Classifier(busi_train, busi_test, Tokenizer, 'cpu')\n",
    "busi_classifier.set_optimizer(LEARNING_RATE)\n",
    "busi_classifier.set_dataset(\"Documents\", \"Categories\", MAX_LEN)\n",
    "busi_classifier.set_dataloader(collate_batch, Train_params, Test_params)\n",
    "busi_train_losses, busi_test_losses = busi_classifier.epoch_train(EPOCHS, 'Busi_BERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training Finished!\n",
      "0 epoch finished!\n",
      "Model Training Finished!\n",
      "1 epoch finished!\n",
      "Model Training Finished!\n",
      "2 epoch finished!\n",
      "Model Training Finished!\n",
      "3 epoch finished!\n",
      "Model Training Finished!\n",
      "4 epoch finished!\n",
      "Model Training Finished!\n",
      "5 epoch finished!\n"
     ]
    }
   ],
   "source": [
    "ent_classifier = BERT_Binary_Classifier(ent_train, ent_test, Tokenizer, 'cpu')\n",
    "ent_classifier.set_optimizer(LEARNING_RATE)\n",
    "ent_classifier.set_dataset(\"Documents\", \"Categories\", MAX_LEN)\n",
    "ent_classifier.set_dataloader(collate_batch, Train_params, Test_params)\n",
    "ent_train_losses, ent_test_losses = ent_classifier.epoch_train(EPOCHS, 'Ent_BERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training Finished!\n",
      "0 epoch finished!\n",
      "Model Training Finished!\n",
      "1 epoch finished!\n",
      "Model Training Finished!\n",
      "2 epoch finished!\n",
      "Model Training Finished!\n",
      "3 epoch finished!\n",
      "Model Training Finished!\n",
      "4 epoch finished!\n",
      "Model Training Finished!\n",
      "5 epoch finished!\n"
     ]
    }
   ],
   "source": [
    "pol_classifier = BERT_Binary_Classifier(pol_train, pol_test, Tokenizer, 'cpu')\n",
    "pol_classifier.set_optimizer(LEARNING_RATE)\n",
    "pol_classifier.set_dataset(\"Documents\", \"Categories\", MAX_LEN)\n",
    "pol_classifier.set_dataloader(collate_batch, Train_params, Test_params)\n",
    "pol_train_losses, pol_test_losses = pol_classifier.epoch_train(EPOCHS, 'Pol_BERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training Finished!\n",
      "0 epoch finished!\n",
      "Model Training Finished!\n",
      "1 epoch finished!\n",
      "Model Training Finished!\n",
      "2 epoch finished!\n",
      "Model Training Finished!\n",
      "3 epoch finished!\n",
      "Model Training Finished!\n",
      "4 epoch finished!\n",
      "Model Training Finished!\n",
      "5 epoch finished!\n"
     ]
    }
   ],
   "source": [
    "sp_classifier = BERT_Binary_Classifier(sp_train, sp_test, Tokenizer, 'cpu')\n",
    "sp_classifier.set_optimizer(LEARNING_RATE)\n",
    "sp_classifier.set_dataset(\"Documents\", \"Categories\", MAX_LEN)\n",
    "sp_classifier.set_dataloader(collate_batch, Train_params, Test_params)\n",
    "sp_train_losses, sp_test_losses = sp_classifier.epoch_train(EPOCHS, 'Sp_BERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training Finished!\n",
      "0 epoch finished!\n",
      "Model Training Finished!\n",
      "1 epoch finished!\n",
      "Model Training Finished!\n",
      "2 epoch finished!\n",
      "Model Training Finished!\n",
      "3 epoch finished!\n",
      "Model Training Finished!\n",
      "4 epoch finished!\n",
      "Model Training Finished!\n",
      "5 epoch finished!\n"
     ]
    }
   ],
   "source": [
    "tech_classifier = BERT_Binary_Classifier(tech_train, tech_test, Tokenizer, 'cpu')\n",
    "tech_classifier.set_optimizer(LEARNING_RATE)\n",
    "tech_classifier.set_dataset(\"Documents\", \"Categories\", MAX_LEN)\n",
    "tech_classifier.set_dataloader(collate_batch, Train_params, Test_params)\n",
    "tech_train_losses, tech_test_losses = tech_classifier.epoch_train(EPOCHS, 'Tech_BERT')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
